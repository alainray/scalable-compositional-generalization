{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estad\u00edsticas de datasets (c y vector D)\n",
    "\n",
    "Este notebook carga un dataset por nombre y calcula estad\u00edsticas b\u00e1sicas usando el split **general\\_composition**,\n",
    "siguiendo la l\u00f3gica del par\u00e1metro **c** y el vector **D** (\"attr\\_difficulty\") usado en los scripts\n",
    "`orthotopic_runner.sh`/`compositional_orth.sh`.\n",
    "\n",
    "Antes de correrlo, aseg\u00farate de descargar los datasets en `data/` (ver README).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from visgen.utils.general import register_resolvers\n",
    "from visgen.datasets import Cars3D, CLEVR, DSprites, IRAVEN, MPI3D, Shapes3D\n",
    "\n",
    "register_resolvers()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_CONFIG_DIR = Path(\"configs/datasets\")\n",
    "\n",
    "DATASET_CLASSES = {\n",
    "    'dsprites': DSprites,\n",
    "    'mpi3d': MPI3D,\n",
    "    'shapes3d': Shapes3D,\n",
    "    'cars3d': Cars3D,\n",
    "    'iraven': IRAVEN,\n",
    "    'clevr': CLEVR,\n",
    "}\n",
    "\n",
    "def parse_attr_difficulty(d_vector):\n",
    "    if d_vector is None:\n",
    "        return None\n",
    "    if isinstance(d_vector, str):\n",
    "        cleaned = d_vector.strip().replace('[', '').replace(']', '')\n",
    "        return [int(x) for x in cleaned.split(',') if x.strip()]\n",
    "    return list(d_vector)\n",
    "\n",
    "def get_attribute_names(dataset, num_attributes):\n",
    "    if hasattr(dataset, '_attribute_indices'):\n",
    "        return [k for k, _ in sorted(dataset._attribute_indices.items(), key=lambda kv: kv[1])]\n",
    "    if hasattr(dataset, '_ATTRIBUTE_INDICES'):\n",
    "        return [k for k, _ in sorted(dataset._ATTRIBUTE_INDICES.items(), key=lambda kv: kv[1])]\n",
    "    return [f'attr_{i}' for i in range(num_attributes)]\n",
    "\n",
    "def load_dataset(name, split='training', c=1, attr_difficulty=None, split_override='general_composition'):\n",
    "    if name not in DATASET_CLASSES:\n",
    "        raise ValueError(f'Unknown dataset: {name}')\n",
    "    cfg = OmegaConf.load(DATASET_CONFIG_DIR / f\"{name}.yml\")\n",
    "    data_cfg = cfg.data[split]\n",
    "    kwargs = {\n",
    "        'path': str(Path(data_cfg.path)),\n",
    "        'dataset_subset': data_cfg.get('dataset_subset'),\n",
    "        'train': bool(data_cfg.train),\n",
    "        'targets': data_cfg.get('targets'),\n",
    "        'split_attributes': data_cfg.get('split_attributes'),\n",
    "        'split': split_override,\n",
    "        'split_difficulty': data_cfg.get('split_difficulty'),\n",
    "        'shuffle': bool(data_cfg.shuffle),\n",
    "        'downsample': int(data_cfg.downsample),\n",
    "        'test_complement': True,\n",
    "        'c': int(c),\n",
    "        'attr_difficulty': parse_attr_difficulty(attr_difficulty),\n",
    "    }\n",
    "    return DATASET_CLASSES[name](**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize_targets(targets):\n",
    "    if targets.ndim == 3 and targets.shape[1] == 1:\n",
    "        return targets[:, 0, :]\n",
    "    if targets.ndim != 2:\n",
    "        raise ValueError(f'Unsupported targets shape: {targets.shape}')\n",
    "    return targets\n",
    "\n",
    "def compute_included_combinations(attribute_values, split_indices, c, threshold_values):\n",
    "    import itertools\n",
    "    split_values = [attribute_values[i] for i in split_indices]\n",
    "    cartesian = np.array(list(itertools.product(*split_values)))\n",
    "    threshold_values = np.array(threshold_values)\n",
    "    included = cartesian[np.sum(cartesian >= threshold_values, axis=1) <= c]\n",
    "    return cartesian, included\n",
    "\n",
    "def analyze_dataset(name, c, d_vector, split='training'):\n",
    "    dataset = load_dataset(name, split=split, c=c, attr_difficulty=d_vector)\n",
    "    targets = normalize_targets(dataset._dataset_targets)\n",
    "    num_samples, num_attributes = targets.shape\n",
    "    attribute_values = getattr(dataset, '_attribute_values', None)\n",
    "    if attribute_values is None:\n",
    "        attribute_values = dataset._get_attribute_values()\n",
    "    attribute_names = get_attribute_names(dataset, num_attributes)\n",
    "\n",
    "    split_attributes = getattr(dataset, '_split_attributes', None)\n",
    "    if not split_attributes:\n",
    "        split_attributes = attribute_names\n",
    "    split_indices = [attribute_names.index(attr) for attr in split_attributes]\n",
    "    threshold_values = parse_attr_difficulty(d_vector)\n",
    "    if threshold_values is None:\n",
    "        raise ValueError('Se requiere un vector D (attr_difficulty) para el split general_composition.')\n",
    "    if len(threshold_values) != len(split_attributes):\n",
    "        raise ValueError(\n",
    "            f'Longitud de D ({len(threshold_values)}) no coincide con split_attributes ({len(split_attributes)}).'\n",
    "        )\n",
    "\n",
    "    cartesian, included = compute_included_combinations(\n",
    "        attribute_values, split_indices, c, threshold_values\n",
    "    )\n",
    "    volume = included.shape[0] / cartesian.shape[0]\n",
    "\n",
    "    summary = {\n",
    "        'dataset': name,\n",
    "        'num_samples': num_samples,\n",
    "        'num_attributes': num_attributes,\n",
    "        'split_attributes': split_attributes,\n",
    "        'c': c,\n",
    "        'D (attr_difficulty)': threshold_values,\n",
    "        'cartesian_size': int(cartesian.shape[0]),\n",
    "        'included_combinations': int(included.shape[0]),\n",
    "        'volume (included/cartesian)': volume,\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame([summary])\n",
    "\n",
    "    attr_stats = []\n",
    "    for idx, name_attr in enumerate(attribute_names):\n",
    "        values, counts = np.unique(targets[:, idx], return_counts=True)\n",
    "        attr_stats.append({\n",
    "            'attribute': name_attr,\n",
    "            'num_values': len(values),\n",
    "            'min_value': int(values.min()),\n",
    "            'max_value': int(values.max()),\n",
    "            'most_common_value': int(values[np.argmax(counts)]),\n",
    "            'most_common_count': int(counts.max()),\n",
    "        })\n",
    "    attr_df = pd.DataFrame(attr_stats)\n",
    "\n",
    "    split_targets = targets[:, split_indices]\n",
    "    split_combo_values, split_combo_counts = np.unique(split_targets, axis=0, return_counts=True)\n",
    "    combo_df = pd.DataFrame({\n",
    "        'combination': [tuple(row) for row in split_combo_values],\n",
    "        'count': split_combo_counts,\n",
    "    }).sort_values('count', ascending=False)\n",
    "\n",
    "    return summary_df, attr_df, combo_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Par\u00e1metros de entrada\n",
    "\n",
    "Define el nombre del dataset, el valor de **c** y el vector **D**.\n",
    "Los valores de **D** deben estar en el mismo orden que `split_attributes` del dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = 'dsprites'\n",
    "c_value = 1\n",
    "d_vector = [2, 3, 14, 14]  # ejemplo para dsprites (c=1)\n",
    "\n",
    "summary_df, attr_df, combo_df = analyze_dataset(dataset_name, c_value, d_vector)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "attr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "combo_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histograma por atributo (primeros 6 atributos para no saturar la visualizaci\u00f3n)\n",
    "plot_attrs = attr_df['attribute'].tolist()[:6]\n",
    "fig, axes = plt.subplots(len(plot_attrs), 1, figsize=(8, 3 * len(plot_attrs)))\n",
    "if len(plot_attrs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "dataset = load_dataset(dataset_name, split='training', c=c_value, attr_difficulty=d_vector)\n",
    "targets = normalize_targets(dataset._dataset_targets)\n",
    "attribute_names = get_attribute_names(dataset, targets.shape[1])\n",
    "\n",
    "for ax, attr in zip(axes, plot_attrs):\n",
    "    idx = attribute_names.index(attr)\n",
    "    values, counts = np.unique(targets[:, idx], return_counts=True)\n",
    "    ax.bar(values, counts)\n",
    "    ax.set_title(f'{attr} (n={len(values)})')\n",
    "    ax.set_xlabel('value')\n",
    "    ax.set_ylabel('count')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}